{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling the Cold Start problem (Support Vector Machines)\n",
    "\n",
    "    Cold start happens when new users or new items arrive in e-commerce platforms.\n",
    "    Classic recommender systems like collaborative filtering assumes that each user or item has some ratings so that we can infer ratings of similar users/items even if those ratings are unavailable.\n",
    "    However, for new users/items, this becomes hard because we have no browse, click or purchase data for them. As a result, we cannot “fill in the blank” using typical matrix factorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "\n",
    "    pandas => Used for storing and basic handling of the data.\n",
    "\n",
    "    numpy => Used for math and logic operations on our data.\n",
    "\n",
    "    sklearn => Used for machine learning models,preparing the input data and evaluating the model performance.\n",
    "\n",
    "    joblib => Used for data serialization.\n",
    "\n",
    "    tkinter => Used for making UI in Python.\n",
    "    \n",
    "    Label Encoder => Used for encoding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import tkinter\n",
    "\n",
    "#----Association Rules -----\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocesing\n",
    "    \n",
    "    Data preprocessing function is used to separate relevant data from the unrelevant and\n",
    "    transform it so that it is ready for SVM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocessing(data):\n",
    "\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    data_green_blue = data.loc[:,'Occurrence: General Liability':'Terrorism']\n",
    "    data_green_blue = data_green_blue.values.tolist()\n",
    "    data_green_blue = np.array(data_green_blue)\n",
    "\n",
    "\n",
    "    return data_green_blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output!](images/tabela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM training\n",
    "    SVM training fuction trains n models,where n is the number of forms.\n",
    "    Input data for each model consists of positive and negative samples.\n",
    "    Positive samples are all the \"users\"(their features) that choose that specific form.\n",
    "    Negative samples are all the \"users\"(their features) that didn't choose that specific form.\n",
    "    \n",
    "    All classifiers get serialized.\n",
    "    Also svm validation accuracies are saved in a dictionary with model numbers as keys.\n",
    "    The accuracy dictionary is also serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVMmodels(data):\n",
    "    #petlja ima iteracija koliko postoji kolona u tabeli\n",
    "    index = 42\n",
    "    num = 1\n",
    "    iterator = 42\n",
    "\n",
    "    for form in range(data.shape[1] - index):\n",
    "        data_pos = data[data.iloc[:,iterator] == 1]\n",
    "        data_neg = data[data.iloc[:,iterator] == 0]\n",
    "\n",
    "        x_pos = dataPreprocessing(data_pos)\n",
    "        x_neg = dataPreprocessing(data_neg)\n",
    "        positive_label = \"Izabrana forma\"\n",
    "        negative_label = \"Nije izabrana forma\"\n",
    "\n",
    "        n = len(x_pos) + len(x_neg)\n",
    "        labels = []\n",
    "        for i in range(n):\n",
    "            if(i < len(x_pos)):\n",
    "                labels.append(positive_label)\n",
    "            else:\n",
    "                labels.append(negative_label)\n",
    "\n",
    "        x = np.vstack((x_pos, x_neg))\n",
    "        y = np.array(labels)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        clf = svm.SVC(kernel='linear')\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_train_pred = clf.predict(x_train)\n",
    "        y_test_pred = clf.predict(x_test)\n",
    "        print(\"Train accuracy: \", accuracy_score(y_train, y_train_pred))\n",
    "        print(\"Validation accuracy: \", accuracy_score(y_test, y_test_pred), '--->' , num )\n",
    "        accuracy_dictionary[num] =  accuracy_score(y_test, y_test_pred)\n",
    "        joblib.dump(clf,'svm_models/clf{}'.format(num))\n",
    "        iterator += 1\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict forms from the given input\n",
    "    Each model will predict if that certain form would be selected or not based on the input information.\n",
    "    Then from positive predictions the ones that have the best accuracy are returned.\n",
    "    \n",
    "    The forms that are predicted are serialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(input_data):\n",
    "        lista = []\n",
    "        result_dict = dict()\n",
    "        lista.append(input_data)\n",
    "        x = np.array(lista)\n",
    "  \n",
    "        num = 1\n",
    "        for m in range(model_n):\n",
    "            clf = joblib.load('svm_models/clf{}'.format(num))\n",
    "            y_test_pred = clf.predict(x)\n",
    "            for label in y_test_pred:\n",
    "                if label == \"Izabrana forma\":\n",
    "                    #print(label + ' ' + form_names[num])\n",
    "                    #result.append(form_names[num])\n",
    "                    result_dict[num] = form_names[num]\n",
    "            num += 1\n",
    "\n",
    "        evaluated = dict()\n",
    "        for key in result_dict.keys():\n",
    "            evaluated[accuracy_dictionary[key]] = form_names[key]\n",
    "\n",
    "        for key in evaluated.keys():\n",
    "            if key > 0.75:\n",
    "                result.append(evaluated[key])\n",
    "\n",
    "\n",
    "        joblib.dump(result,'cold_start_output')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main :\n",
    "    Input data has 25 elements for 25 claims and limitations that user can select or not.\n",
    "    Tkinter UI lets the user to chose whichever they want.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended forms:\n",
      "--------------\n",
      "MEIL 1231 10 13\n",
      "IL 00 21 09 08\n",
      "MEIL 1200 10 16\n",
      "MDGL 1000 01 13\n",
      "CG 00 01 04 13\n",
      "ME 037 04 99\n",
      "MEGL 0008 01 16\n",
      "MEGL 0219 05 16\n",
      "MEGL 1361 05 16\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('BC - AI ORIGINAL.xlsx', sheet_name= None)\n",
    "data = data['BC - AI - V1']\n",
    "model_n = data.shape[1] - 42\n",
    "dictionary_keys = []\n",
    "\n",
    "for i in range(model_n):\n",
    "    dictionary_keys.append(i)\n",
    "\n",
    "accuracy_dictionary = joblib.load('accuracy_dictionary')\n",
    "\n",
    "#trainSVMmodels(data)\n",
    "#joblib.dump(accuracy_dictionary,'accuracy_dictionary')\n",
    "\n",
    "forms = data.loc[:,\"MJIL 1000 08 10\":\"MIL 1214 09 17\"]\n",
    "\n",
    "form_names = []\n",
    "for col in forms.columns:\n",
    "    form_names.append(col)\n",
    "\n",
    "\n",
    "result = []\n",
    "model_n = data.shape[1] - 42\n",
    "\n",
    "input_data = [0,1,0,0,0,1,0,0,0,1,1,1,1,0,0,0,0,0,1,1,0,1,0,0,0]\n",
    "\n",
    "predict(input_data)\n",
    "result = joblib.load('cold_start_output')\n",
    "\n",
    "\n",
    "print(\"Recommended forms:\")\n",
    "print(\"--------------\")\n",
    "for r in result:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Final output is a list of recommended forms and that is the end of step one, which is solving the\n",
    "    cold start problem.\n",
    "  \n",
    " Next step is predicting more forms based on which forms user selected from the list (explicit user interaction) using association rules.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filltering using Association Rules\n",
    "\n",
    "         Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).\n",
    "                \n",
    "         Association rule mining, at a basic level, involves the use of machine learning models to analyze data for patterns, or co-occurrence, in a database. It identifies frequent if-then associations, which are called association rules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Powersets\n",
    "        The get_subsets function makes a powerset from a list and sorts it based on the number of elements,in a descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sorting(lst):\n",
    "    lst2 = sorted(lst, key=len)\n",
    "    return lst2\n",
    "\n",
    "def get_subsets(fullset):\n",
    "    listrep = list(fullset)\n",
    "    subsets = []\n",
    "    for i in range(2**len(listrep)):\n",
    "        subset = []\n",
    "        for k in range(len(listrep)):\n",
    "            if i & 1<<k:\n",
    "                subset.append(listrep[k])\n",
    "        subsets.append(subset)\n",
    "\n",
    "    subset = Sorting(subsets)\n",
    "    subset = list(reversed(subset))\n",
    "\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "    This function prepares the data and makes an input for fp growth to find patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_fpgrowth(data):\n",
    "    input_data = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        dict_values_list = []\n",
    "        for i in range(len(row.values)):\n",
    "            if row[i] == 1:\n",
    "                dict_values_list.append(row.index[i])\n",
    "\n",
    "        dict_values_list = le.transform(dict_values_list)\n",
    "        input_data.append(dict_values_list)\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules Analyzing\n",
    "        This functions analyzes association rules and returns recommended forms for the given input.\n",
    "  \n",
    "  Input is a list of user selected forms from the step one (\"cold_start_output\")\n",
    "  \n",
    "### Rules mining algorythm:\n",
    "  \n",
    "  1. The list is transformed into a powerset.\n",
    "  2. In keys of association rules dictionary we try to find the first element (element with the biggest length) of the powerset.\n",
    "  3. If we find it, we save the values for that key.\n",
    "  4. If not, we do the same thing with other elements of the subset that are smaller in length then the first element.\n",
    "  5. Repeat        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_analyzer_and_predictor(final_input):\n",
    "    subset = get_subsets(final_input)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    s = subset.pop(0)\n",
    "\n",
    "    s = le.transform(s)\n",
    "    s = tuple(s)\n",
    "    input_is_key = rules.get(s)\n",
    "    if input_is_key:\n",
    "        value = rules[s]\n",
    "        output = le.inverse_transform(value[0])\n",
    "        output = output.tolist()\n",
    "        result.append(output)\n",
    "    else:\n",
    "        num = 0\n",
    "        for s in subset:\n",
    "            s = le.transform(s)\n",
    "            s = tuple(s)\n",
    "            if num == 0:\n",
    "                set_len = set_the_length_of_the_current_set(s, result)\n",
    "                num += 1\n",
    "            else:\n",
    "                if set_len == len(s):\n",
    "                    input_is_key = rules.get(s)\n",
    "                    if input_is_key:\n",
    "                        value = rules[s]\n",
    "                        output = le.inverse_transform(value[0])\n",
    "                        output = output.tolist()\n",
    "                        result.append(output)\n",
    "                        num += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    if not result:\n",
    "                        set_len = set_the_length_of_the_current_set(s, result)\n",
    "\n",
    "    temp = []\n",
    "    for res in result:\n",
    "        for r in res:\n",
    "            temp.append(r)\n",
    "\n",
    "    ctr = collections.Counter(temp)\n",
    "\n",
    "    # sort dictionary by frequency\n",
    "    {k: v for k, v in sorted(ctr.items(), key=lambda item: item[1])}\n",
    "    print(\"Recommended forms:\")\n",
    "    print(\"--------------\")\n",
    "    for form in ctr:\n",
    "        print(form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_the_length_of_the_current_set(set,result):\n",
    "    set_len = len(set)\n",
    "    input_is_key = rules.get(set)\n",
    "    if input_is_key:\n",
    "        value = rules[set]\n",
    "        output = le.inverse_transform(value[0])\n",
    "        output = output.tolist()\n",
    "        result.append(output)\n",
    "    return set_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main : \n",
    "      First, Label Encoder is trained on all labels of all forms.\n",
    "      Then FP GROWTH is used to find patterns in all the data regarding form iteraction.\n",
    "      From those patterns, rules are formed using \"generate_association_rules\" function.\n",
    "      Rules are in the form of a python dictionary.\n",
    "    \n",
    "      We use the rules_analyzer_and_predictor function to predict more rules from the given input.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended forms:\n",
      "--------------\n",
      "MDIL 1000 08 11\n",
      "MDIL 1001 08 11\n",
      "MEIL 1225 10 11\n",
      "MJIL 1000 08 10\n",
      "MPIL 1007 03 14\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "data = data.loc[:,'MJIL 1000 08 10':'MIL 1214 09 17']\n",
    "\n",
    "for d in data:\n",
    "    labels.append(d)\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "\n",
    "input_data = prep_data_for_fpgrowth(data)\n",
    "\n",
    "\n",
    "#patterns = find_frequent_patterns(input_data,support_threshold = 280)\n",
    "#rules = fpg.generate_association_rules(patterns,0.7)\n",
    "#print(len(rules))\n",
    "#joblib.dump(rules,'serialized_model/rules')\n",
    "\n",
    "\n",
    "rules = joblib.load('serialized_model/rules')\n",
    "\n",
    "recommended_forms = joblib.load('cold_start_output')\n",
    "\n",
    "final_input = ['IL 00 17 11 98', 'MEIL 1200 10 16', 'MEIL 1231 10 13', 'MEGL 0008 01 16', 'MEGL 0219 05 16']\n",
    "\n",
    "rules_analyzer_and_predictor(final_input)\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
